\section{Quick wins: from Make to Shake\label{sec:solutions}}

The GHC build system stumbles into a number of nasty corners of Make. In this
section we reflect on the challenges, and how they can be solved in
our new system. We consider these \empty{unnecessary} complexities --
they are not consequences of our problem domain, merely weaknesses of Make on
large projects. The problems can be divided into those caused by the
Make language (macros, variables, etc.) and those caused by a lack of expressive
power when describing dependencies. We tackle these problems using
functional programming for the language level, and Shake for the dependency
level. After tackling these unnecessary complexities, we show how to structure a
large build system in~\S\ref{sec:abstractions}.

\subsection{Programming model}

Make's program state involves a global namespace of mutable string variables which are
spliced into the program. This model naturally causes challenges:

\begin{itemize}
\item Since variables live in a single global namespace, there is limited
encapsulation and implementation hiding.
\item Since variables are strings, arrays and associatve maps are typically
encoded using computed variable names, which is error-prone.
\item Since variables are mutable, the time at which they are expanded needs
to be carefully controlled, resulting in constructs such as \lst'$$$$foo' to
delay expansion until after interpretation by two macros.
\item Since variable references are spliced into the Makefile contents
and interpretted, certain special characters can cause problems --
notably space (which splits lexemes) and colon (which declares rules, but is
also used in Windows drive letters).
\end{itemize}

The solution to encapsulation is a properly block-scoped language with implementation
hiding, e.g. modules in Haskell. The solution to excessive
freedom is types (not necessarily static). The solution to
splicing is to separate values and program text, much like most programming
languages do. By using Haskell, or indeed any other modern programming language,
most of these issues are eliminated.

An alternative solution is to generate the Makefile, either using a tool such as
Automake or a full programming language. However, a generator cannot interact with the
build system after generation, resulting in problems with dynamic dependecies
(\S\ref{sec:dynamic-deps}).

\subsection{Pattern/rule language\label{sec:pattern-rule-language}}

A build system builds some output files from some input files. The fundamental
unit of work in Make (and also Shake) is a rule that produces some output by
running commands on some input. Consider the following Make pattern rule:

\begin{lstlisting}
%.o : %.hs
    ghc $HC_OPTS $<
\end{lstlisting}

\noindent It tells Make that object files \lst"*.o" can be produced from Haskell
source files \lst"*.hs" by compiling them with \lst"ghc" command invoked with
\lst"HC_OPTS" arguments. The notation is terse and works well for this simple
case. Unfortunately, this simple formulation does not support numerous important
features, for example:

\begin{itemize}
\item What if we want the rule to match \lst"foo.o" and \lst"bar.o", but not
\lst"baz.o"? It is impossible to do any non-trivial computation -- we are forced
to rely on patterns whose expressive power is limited.
\item What if \lst"HC_OPTS" should depends on the file being compiled?
\end{itemize}

The standard approach to overcome some of Make's limitations is to use
\emph{macros}. They provide some additional flexibility, but quickly become
difficult to follow. As an example, here is a rule derived from the example
shown in~\S\ref{sec:challenges}:

\begin{lstlisting}
$1/$2/build/%.o : $1/$4/%.hs $$$$($1_$2_HC)
    $1_$2_HC $$($1_$2_$3_HC_OPTS) -c $$< -o $$@
\end{lstlisting}

\noindent As before, the rule is responsible for compiling a Haskell source
file into an object file. The plethora of dollars works like this:
\begin{itemize}
\item Arguments of the macro are \lst"$1" to \lst"$4".

\item GNU Make has a single global namespace for variables, and yet we
  have a single build system that encompasses many similar components.
  To keep things sane, we have to prefix all our variables with the
  directory name, and because we build multiple instances of things
  (e.g. two stages of the compiler), we have yet more prefixes.  There
  are three prefixes in total, by convention we tend to use \lst"$1",
  \lst"$2", and \lst"$3".

  In this case we pick the right Haskell compiler \lst"$1_$2_HC"
  and run it with command line arguments \lst"$1_$2_$3_HC_OPTS".

\item Expanding a variable before the variable is defined gives an
  empty result.  Thus, if we're defining a macro before the variables
  it refers to, we need to use a double dollar (\lst"$$") to delay the
  expansion until the macro is called.

\item Sometimes we need to delay the
  expansion by yet another step, requiring a double-double-dollar (\lst"$$$$").
  Deciding how many dollars to use in any given place is hard.
\end{itemize}

\noindent Using Shake the simplest variant looks slightly more complex, because
it must be expressed in Haskell syntax:

\begin{lstlisting}
"*.o" %> \out -> do
    let hs = out -<.> "hs"
    need [hs]
    cmd "ghc" hc_opts hs
\end{lstlisting}
\noindent
However, as the complexity grows, so the build system scales properly. Making
\lst"hc_opts" depend on the Haskell file requires the small and obvious change of:

\begin{lstlisting}
    cmd "ghc" (hc_opts hs) hs
\end{lstlisting}
\noindent
To use richer pattern matching we can drop down to a lower level Shake
operation. In Shake the definition of is \lst"%>" is itself defined in terms of
\lst"?>", as:

\begin{verbatim}
pat %> act = (pat ?==) ?> act
\end{verbatim}
\noindent
So the wildcard pattern matching is just a special case, and we can use an
arbitrary predicate to exert more precise control over where the match occurs.
In both cases the generality of higher-order functions solves the problem.

\subsection{Generating rules}

\todo{\textbf{Andrey}: Rewrite.}

In any large build system, there are rules (how to build things), and then a
very long list of corner cases. For example,

\begin{lstlisting}
compiler_ALEX_OPTS = --latin1
\end{lstlisting}

While this is a concise way of representing it, it's inflexible. Rather than
pattern matching on everything, we are pattern matching on two components (which
package we are building and which tool we are running). Adding additional
filters is impossible.

Implementing the DSL is a nightmare. It's really a configuration setting.

The question of provenance is particularly important in large multi-author build
systems, but generally not something that is addressed anywhere - one of our key
inovations in \S?. The encapsulation is provided by Haskell modules. The lack of
freedom is provided by strong name binding. The problems related to `:' go away
by manipulating values rather program text.


As we develop more Shake-based build systems, patterns have started to emerge.
Typically 90\% of a build system can be captured in some simple DSL, taking
advantage of conventions, and 10\% cannot. As an example, a large build system
might build 100 C++ libraries, each with similar flags and file layout, but
taking the source files from different directories. It may also minify a
Javascript file, and build an installer -- both one-off tasks. Using a
fully-powerful system such as Shake, it is possible to engineer robust
abstractions, and then define the majority of the build system using only these
abstractions. The end result is that most edits to the build system involve only
the DSL, and can be performed by a large number of individuals.

After dealing with the DSL, there are usually a few pieces left over, and these
can be implemented in Shake, as normal. Thanks to the power of Shake you can
interpret the DSL and combine it with the custom pieces. In our experience by
providing an \emph{escape hatch} where fully-powerful code an be expressed it
removes the temptation to shoehorn more advanced features in the DSL, and thus
avoids turning it into an ad-hoc scripting language. Instead, should enough
pieces be required in the escape hatch, they can be abstracted in the
traditional ways - perhaps even combining two DSLs in one build system.

\subsection{Rules with multiple outputs\label{sec:multiple-outputs}}

GHC produces object \lst"*.o" and interface \lst"*.hi" files when compiling
Haskell sources. How do we express this in a build rule?

A typical Make appoach to this is to use two rules, one depending on the other.
For example:

\begin{lstlisting}
%.o : %.hs
    ghc $HC_OPTS $<
#\vspace{-2mm}#
%.hi : %.o ;
\end{lstlisting}
\noindent Here \lst';' is a no-op: it tells Make that an interface file can be
produced simply by depending on the corresponding object file. Alas, this
approach is fragile: if one deletes the interface file, but leaves the object
file intact, Make will not rerun the \lst'*.o' rule, because the object file is
up-to-date. Consequently, the build system will fail to restore the deleted
interface file.

In Shake we can express this rule directly using operator \lst'&%>',
which defines a build rule with multiple outputs:

%(it actually turns out to be a
%consequence of the more powerful dependency model):

\begin{lstlisting}
["*.o", "*.hi"] &%> \[o, hi] -> do
    let hs = o -<.> "hs"
    need [hs]
    cmd "ghc" hc_opts hs
\end{lstlisting}

\subsection{Reducing concurrency\label{sec:ghc-pkg-db}}

As discussed in~\S\ref{sec:concurrency-reduction}, it is sometimes necessary to
reduce concurrency in a build system. As one example, GHC packages need to
be registered by invoking the \lst'ghc-pkg' utility. This utility mutates the
global state (package database) and hence at most one package can be registered
at a time, or the database is corrupted.

In Make, the solution is to introduce fake \emph{concurrency
reduction} dependencies. There are 25 GHC packages that require registration,
and in the old build system they all depended on each other in a chain, to
ensure that no simultaneous registrations occur. This solution works, but is
inelegant and inefficient.

There are really three problems. By over-sequentialising things we sacrifice
other parallelism that may be available in the system. We must over-constrain to
give a precise order in which the rules can be built. If one of the later ones
is available to build, we may need to stop it by waiting for another. Finally,
it's easy to miss such dependencies as they are not centralised. By accidentally
turning the chain into a tree we can end up with surprising failures.

In Shake, the solution is to use the \emph{resources} feature:

\begin{lstlisting}
packageDb <- newResource "package-db" 1
action $ withResource packageDb 1 $ cmd "ghc-pkg" ...
\end{lstlisting}

This snippet declares a global resource named \lst"packageDb" with quantity 1,
then calls \lst"withResource" asking for a single quantity before running the
\lst'ghc-pkg' utility. Provided all \lst'ghc-pkg' calls are suitably wrapped,
we will never run two instances simultaneously. Furthermore, thanks to the
availability of functions, we can abstract a function that executes
\lst'ghc-pkg' and applies the resource automatically, see
\S\ref{sec:abstractions} for how we do that.
% Note that build systems already schedule tasks to satisfy the implicit thread
% resource, so having user defined resources is an obvious generalisation.

\subsection{Dynamic dependencies\label{sec:dynamic-deps}}

Make, in common with many other build systems, works by
constructing a dependency graph and then executing it. This approach
has some benefits in that it is possible to analyse the graph ahead of
time, but it is limiting in some important ways.  It's common to run
into cases where parts of the dependency graph can only be known after
executing some other parts of the dependency graph.  Some examples of
this that occur in the GHC build system:

\begin{itemize}
\item GHC contains Haskell packages which have metadata specified in
  \lst'.cabal' files. We need to extract the metadata and generate
  \lst'package-data.mk' files to be included into the build system; this
  is done by a Haskell program called \lst'ghc-cabal', which is built by the
  build system. Hence we do not know how to build the packages until we have
  built and run the \lst'ghc-cabal' tool.

  One ``solution'' to this problem is to generate the \lst'.mk'
  files and check them into the repository whenever they change, but
  checking in generated files is ugly and introduces more failure
  cases when the generated files get out of sync.

\item The dependencies of a Haskell source file can be extracted by
  running the compiler, much like using \lst'gcc -M' on a C source
  file.  But the Haskell compiler is one of the things that we are
  building, so we cannot know the dependencies of many of the Haskell
  source files until we have built the stage 1 compiler.

\item Source files processed by the C preprocessor using
  \lst'#include' directives to include other files.  We can only
  know what the dependencies are by running the C preprocessor to
  gather the set of filenames that were included.
\end{itemize}

There are several other cases of such \emph{dynamic dependencies} in the build
system. Indeed, they arise naturally even when building a simple C program,
because the \lst'#include' files for a C source file are not known until the
source file is compiled or analysed by the C compiler, using something like
\lst'gcc -M'. Build systems that have a static dependency graph typically have
special-case support for this; for example, GNU Make will re-read
\lst'Makefiles' that change during building.  In the case of GNU Make,
this works for simple cases, but fails when there are dependencies
between the generated \lst'Makefiles', which arises in more complex cases.

The GHC build system works around this limitation of GNU Make by
dividing the build system into \emph{phases}, where each phase
generates the parts of the build system that are required by
subsequent phases. We managed to reduce the number of phases to
three, by carefully making use of GNU Make's automatic restarting
functionality where possible. However, explicit phases are a terrible
solution for several reasons:

\begin{itemize}
\item Phases reduce concurrency: we cannot start the next phase until
  the previous one is complete.
\item Knowing what targets to put in each phase requires a deep
  understanding of what the generated parts of the build system are
  and their dependencies, and this area of our build system is
  notoriously difficult to work on.  Diagnosing problems is
  particularly hard.
\item Even when doing an incremental build we have to perform all the
  phases, because explicit phases preclude dependency tracking across
  the phase boundary.  This means we have to work around slow
  incremental builds by having an explicit way for the user to say
  that they are sure nothing has changed that would require rebuilding
  the early phases (\lst'make fast').  Obviously this is less than
  ideal.
\end{itemize}

Shake supports dynamic dependencies natively, so these problems just
go away.  The \lst"need" function can introduce new dependencies
\emph{after observing the results of previous dependencies}\footnote{It has been
suggested that the dependency mechanism in Shake should rightly be called
\emph{Monadic dependencies} to contrast with the \emph{Applicative dependencies}
in Make. We agree. In an applicative computation the structure cannot depend on
the values flowing through container. In a monadic computation the structure can
depend on the values.}. For example, when building a Haskell package, we first
\lst"need" the \lst'ghc-cabal' tool, then we run it on the package's \lst'.cabal' file to
extract the package metadata, and then we consult the result to determine what
needs to be done to build the package.

The existence of \lst"need" means that Shake cannot construct the
dependency graph ahead of time, because it doesn't know the full
dependency graph until the build has completed, but this is not a
limitation in practice (while the lack of dynamic dependencies really
\emph{is} a limitation requiring painful workarounds).

% SimonM: I wasn't really sure what this paragraph was trying to say
% While the existence of the \lst"need" fundamentally alters the expressive
% power of Shake, for users it is merely a relaxation of an unnecessary rule
% (that \lst"need" must be the in a rule), so just makes things easier. At the
% build system level it requires many changes, the lack of a static dependency
% graph, termination checking as it executes, the ability to suspend partially
% executed rules etc. Fortunately, as users we do not need to care.

Note, Redo (\S\ref{sec:redo}) is the only other clearly monadic build
system, while it can be encoded in SCons using the dependency rules.

% \subsection{Fine-grain dependencies\label{sec:fine-grain-deps}}
% 
% There are many instances where an action depends on a part of a file. As a
% compelling example in GHC, there are many configuration settings provided in a
% file \lst"system.config", which reads:
% 
% \begin{lstlisting}
% system-ghc = /usr/bin/ghc
% system-gcc = /usr/bin/gcc
% ...
% \end{lstlisting}
% 
% \vspace{-2mm}
% When executing a command we typically depend on only a handful of lines in this
% large configuration file. However, using simple file dependencies, we have to
% depend on the whole file. As a result, we end up rebuilding more than is
% necessary (see~\S\ref{sec:ghc}).
% 
% One solution is to create a single file for each setting, but in GHC the whole
% \lst"system.config" file is generated by the \lst'configure' script, which makes
% this challenging. Since a single file is being generated, the next best solution
% is to chop it up into multiple small files, to provide accurate fine-grained
% dependencies.
% 
% This approach does not work in Make. We can produce individual
% files, but the individual files must all depend on \lst"system.config", so they
% update appropriately. In the rule to generate the fragments, if we always generate the
% new file, then we do not break the dependencies -- everything still has an
% ultimate dependency on the whole file. If in a rule we chose to avoid generating
% the small file then the file always looks dirty (since its timestamp is older
% than that of \lst"system.config", which Make considers to signify dirty).
% 
% Fortunately, Shake supports polymorphic dependencies that solve this problem,
% as discussed in~\S\ref{sec:polymorphic}. This brings dramatic improvements -- on
% real build systems we regularly see build times of a couple of seconds, which
% would have been minutes without this feature (e.g., see~\S\ref{sec:ghc}). This
% feature is also available in \lst'Tup' and \lst'Ninja', although in both cases
% requires explicitly enabling per rule.

% (using \lst'stat' in \lst'Ninja', and \texttt{\^} in Tup).

% Using the same approach in Shake we can simply avoid writing out the small file
% if it has not changed. Shake considers a file to be rebuilt if the rule runs
% successfully, but only considers the file \emph{changed} if its value changed.
% As a result, Shake can start down a build tree and then later avoid that build
% tree. This feature is built into the core of Shake, and means that, unlike
% Make, we can split a single dependency into fine-grained dependencies
% successfully.

% \subsection{Everything is a file}
%
% Once fine grained dependencies are available, they can quickly become
% pervasive. Using a build system based on filenames as keys and files as values
% we effectively reach the global store problem we criticised in \S? (particularly
% if we consider dyanmic dependencies from \S? to be \lst"$" dereferencing!).
% While not critical, Shake supports an Oracle feature, to allow us to make the
% keys and values arbitrary Haskell types, and store them in the main Shake
% database. Thanks to such a technique we can have an oracle per rule we create to
% track changing information, without necessarily doubling the number of files
% present in the build system. As an example:
%
% \begin{lstlisting}
% newtype ConfigKey = ConfigKey String
%     deriving (Show,Typeable,Eq,Hashable,Binary,NFData)
% rules = do
%     addOracle $ \(ConfigKey x) -> do
%         src <- readFile' "system.config"
%         Map.lookup (parseFile src) x
%
%     ... askOracle (ConfigKey x) ...
% \end{lstlisting}
%
% Here we define our own rule of type \lst"ConfigKey" which reads from the
% configuration and stores just that one field. Since this also participates in
% unchanging rules we automatically get rebuild avoidance if the value does not
% change.

\subsection{Real code\label{sec:real_code}}

Much of a build system is calling out to programs that execute real code, e.g.
compilers. But there is some stuff that requires custom code, for example
splitting command line arguments that exceed a certain size. In the old
build system we used \lst'xargs', which alas does not work consistently between
different OS versions, and does both more than we want and less than we want.
Fortunately, with Haskell at our disposal, we can write:

\begin{lstlisting}
-- | @chunksOfSize size strings@ splits a given list of strings
-- into chunks not exceeding @size@ characters. If that is
-- impossible, it uses singleton chunks.
chunksOfSize :: Int -> [String] -> [[String]]
chunksOfSize n = repeatedly $ \xs ->
    let#\,#i = length $ takeWhile#\,#(<=#\,#n) $ scanl1#\,#(+) $ map#\,#length#\,#xs
    in splitAt (max 1 i) xs
\end{lstlisting}

\noindent Writing a small function is easy in Haskell, and even easier to test
(we test this function using QuickCheck). Writing it in \lst'bash' would be infeasible.
Reducing the specific behaviours required from external tools leads to
significantly less cross-platform concerns.

\subsection{Summary}

The unnecessary complexities described in this Section have a big consequence in
terms of complexity and performance. They obscure the underlying real stuff in
the build system. Simply removing these already gets us to a much better state.

The main lessons we have learnt are:

\begin{itemize}
\item Abstraction is a powerful and necessary tool. Lack of good abstraction
mechanisms is a significant cause of the complexity of previous attempts in
Make. Functional programming provides good abstractions which in turn allow
reducing the complexity at each level, while still obtaining a more powerful
result. Marrying build systems and functional programming works well.
\item Use a build system that can express the necessary dependencies. We make
use of multiple outputs~\S\ref{sec:multiple-outputs},
resources~\S\ref{sec:ghc-pkg-db}, dynamic dependencies~\S\ref{sec:dynamic-deps},
fine-grain dependencies~\S\ref{sec:fine-grain-deps}. While some of these
features are only used in a few places (e.g., resources), their absence would
require pervasive workarounds, and a significant increase in the overall complexity.
\end{itemize}

% \subsection{Abstracting over patterns of dependencies}
%
% \todo{This was moved from Section 5.}
%
% By embedding the build system in Haskell we get access to its rich abstraction
% facilities. Examples:
% \begin{itemize}
%   \item Use general predicates instead of file patterns. For example, several
%   files are built by calling \texttt{genprimopcode}, which can be captured by a
%   single build OR-rule:
%
% \begin{lstlisting}
% [ "autogen/GHC/Prim.hs"
% , "GHC/PrimopWrappers.hs"
% , "*.hs-incl" ] |%> \file -> do ...
% \end{lstlisting}
%
%   \item Build rules with multiple outputs (AND-rules)
%   \item Compute build arguments from target filename
% \end{itemize}
