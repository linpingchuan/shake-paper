
\section{Incidental complexities\label{sec:challenges}}

The GHC build system stumbles into a number of nasty corners of \make{}. In this section we reflect on the challenges, and how they can be solved generically in our new system. We very much consider these \textit{unnecessary} complexities -- they are not consequences of our problem domain, merely weaknesses of \make{} on large projects. In general the problems can be divided into those due to the \make{} language (macros, variables etc.) and those due to the \make{} dependency features (lack of expressive dependencies). Consequently, we tackle these problems using functional programming for the language level, and Shake for the dependency level. After tackling these unnecessary complexities, we show how to construct a large build system avoiding these issues in \S\ref{sec:abstractions}.

\subsection{Programming model}

\make{}'s program state involves a global namespace of string variables which are spliced into the program. This model naturally causes challenges:

\begin{itemize}
\item Since all variables live in a single global namespace, there is limited encapsulation and implementation hiding. While macros introduce some level of scope, their use leads to different problems.
\item Excessive freedom, implementing \texttt{\$\$\$foo} to do multiple lookups. Such code is much like \texttt{eval} in a scripting language, but it is unlikely any scripting language author would encourage such a level of indirection. Alas, without real data structures (associative maps, arrays) such tricks are necessary.
\item Since the text is spliced into the resulting Makefile to be interpreted, certain characters can pose lots of problems -- notably space (which splits lexemes which might have been intended to be joined) and `:' (which declares rules, but often tends to be a Windows drive letter gone astray). Much like command line quoting, with sufficient care such issues can be avoided -- but it does require constant care.
\item A string variable, such as \texttt{\$1\_\$2\_\$3\_HC\_OPTS}, can be manipulated in many different files and it is very difficult to track the provenance of a specific command line argument that is eventually passed to \texttt{ghc}.
\end{itemize}

The solution to the issue of encapsulation is a properly block-scoped language with separate units of implementation hiding, e.g. modules in Haskell. The solution to excessive freedom is types (not necessarily static). The solution to splicing is to separate values and program text, much like most programming languages do. By using Haskell, or indeed any other modern programming language, most of these issues are solve.

An alternative solution would be to avoid putting complexity into the Makefile, and place it in a generator of a Makefile (much like the Ninja build system does). Alas, such a solution runs into problems if there is a dynamic call graph, as per \S?.

At this point it is worth remarking that many alternative build systems do indeed offer real programming language embedding, e.g. SCons (Python). We address the specific limitations that drove us away from them in later sections, and discuss all alternatives in \S?.

\subsection{Pattern/rule language}

A build system builds some output files from some input files. The fundamental unit of work in \make{} (and also Shake) is a rule that produces some output by running commands on some input. Consider the following pattern rule:

\begin{lstlisting}
%.o : %.hs
    ghc $HC_OPTS $<
\end{lstlisting}

\noindent It tells \make{} that object files \lst"*.o" can be produced from Haskell source files \lst"*.hs" by compiling them with \lst"ghc" command invoked with \lst"HC_OPTS" arguments. The notation is terse and works well for this simple case. Unfortunately, this simple formulation does not support numerous important features, for example:


 but as soon as one goes beyond \textit{one rule
fits all} the following limitations are encountered:
\begin{itemize}
  \item What if we want the rule to match \texttt{foo.o} and \texttt{bar.o}, but
  not \texttt{baz.o}? It is impossible to do any non-trivial computation and
  we have to rely on patterns whose expressive power is limited.
  \item What if \texttt{HC\_OPTS} depends on the file and/or something else?
\end{itemize}

\newsavebox{\exampleCode}
\begin{lrbox}{\exampleCode}
\begin{minipage}[t]{\columnwidth}
Here is the code in full glory. We cannot go into details due to lack of
space, but would like to point out that there are \$57 in three lines!
\begin{lstlisting}[basicstyle=\footnotesize\sffamily,escapeinside={(*}{*)}]
$1/$2/build/%.$$($3_osuf) : $1/$4/%.hs $$(LAX_DEPS_FOLLOW) \
    $$$$($1_$2_HC_DEP) $$($1_$2_PKGDATA_DEP)
  $$(call cmd,$1_$2_HC) $$($1_$2_$3_ALL_HC_OPTS) -c $$< -o \
    $$@ $$(if $$(findstring YES,$$($1_$2_DYNAMIC_TOO)),-dyno \
    $$(addsuffix .$$(dyn_osuf),$$(basename $$@)))
  $$(call ohi(*-*)sanity(*-*)check,$1,$2,$3,$1/$2/build/$$*)
\end{lstlisting}
\end{minipage}
\end{lrbox}

The standard approach to overcome some of \textsc{Make}'s limitations is to use
\emph{macros}. They provide more flexibility but tend to become Utterly
Impenetrable as demonstrated by the following \emph{significantly simplified}
snippet\footnote{\usebox{\exampleCode}} from the GHC build system:

\begin{lstlisting}
$1/$2/build/%.o : $1/$4/%.hs $$$$($1_$2_HC)
    $1_$2_HC $$($1_$2_$3_HC_OPTS) -c $$< -o $$@
\end{lstlisting}

\noindent As before, the rule is responsible for compiling a Haskell source
file into an object file. Arguments of the macro (\texttt{\$1-\$4}) provide
additional information about the current build target so that we could pick the
right Haskell compiler \texttt{\$1\_\$2\_HC} and run it with appropriate command
line arguments \texttt{\$1\_\$2\_\$3\_HC\_OPTS}.

By embedding the build system in Haskell we get access to its rich abstraction
facilities. See Section~\ref{section-abstractions} for more details.

We solve this using Haskell functions. In Shake there is \verb"%>", which itself is defined in terms of \verb"?>", as:

\begin{verbatim}
pat %> act = (pat ?==) ?> act
\end{verbatim}

Namely the wildcard pattern matching is just a special case, so we can revert to more precise control mechanisms where it makes sense.

\subsection{Multiple outputs}

In the rule above, GHC actually produces two files: \texttt{*.o} and \texttt{*.hi}. How do we express this?

Mostly by pretending the .hi file depends on the .o file, which is (mostly) true. But only really because we always touch the .o file.

Using Shake we can express this pattern directly (it actually turns out to be a consequence of the more powerful dependency model).

\subsection{Controlling concurrency}
The only way to control build concurrency in \textsc{Make} is by introducing
fake \emph{concurrency reduction} dependencies.

\textbf{Example:} Some GHC packages need to be registered by invoking
\texttt{ghc-pkg} utility. The utility mutates the global state (package
database) and hence at most one package can be registered at a time. In order to
enforce sequential package registration in the old build system it is necessary
to manually introduce fake dependencies between packages. Not only this is an
an error prone and mundane task, the resulting performance is suboptimal: only
one predefined registration sequence is allowed and packages are required to
wait for their turn even if no registration is currently taking place.
\textsc{Shake} provides fine-grain mechanisms for controlling concurrency, as
discussed in Section~\ref{section-abstractions}.

There are really three problems. By over-sequentialising things we limit parallelism that may be in the system. We cannot say that all pieces should be run single-threaded, we must over-constrain to give a precise order in which they can build. If one of the later pieces is free to build, we may stop it by waiting for another. Finally, it's easy to miss such dependencies as they are not centralised. By turning the distributed line into a tree we can end up with surprising failures.

We solve this using Shake resources. A Shake resource limits parallelism by declaring that a certain task requires some quantity of resource. For example:

\begin{verbatim}
r <- newResource 1
withResource r 1 $ ...
\end{verbatim}

\subsection{Dynamic dependencies}

Or, perhaps more generally: \emph{Accurate dependencies}. Why accurate
dependencies are important? 1) Performance. 2) Correctness. Anything else?

The core of the challenge: the dependency graph is not static and it is not
known in advance.

Two specific instances of the accurate dependencies challenge are: \emph{dynamic
dependencies} and \emph{fine-grain dependencies}.

We handle dynamic dependencies using \textsc{Shake}'s oracles. Examples: import
and include dependencies.

\textsc{Make} has no support for \emph{dynamic dependencies}. Build
phases do not work in general. Below is a representative dependency chain:



\begin{itemize}
  \item File \texttt{compiler/prelude/primops.txt.pp} describes GHC's primitive
  operations and types. It uses C preprocessor directives like
  \texttt{\#include} and \texttt{\#if} and therefore needs to be preprocessed.
  The result goes into \texttt{compiler/stage1/build/primops.txt}. If one of the
  included files changes the result must be rebuilt. Note: we do not know in
  advance which files are included, so we cannot depend on them statically,
  that is why such dependencies are called dynamic.
  \item File \texttt{primops.txt} is processed by \texttt{genprimopcode} utility
  to generate \texttt{primop-out-of-line.hs-incl}. \texttt{genprimopcode} itself
  needs to be built. It is a Haskell program, so it may contain \texttt{import}
  directives, which also give rise to dynamic dependencies.
  \item A dozen of \texttt{*.hs-incl} files are included (by a C preprocessor)
  into \texttt{compiler/prelude/PrimOp.hs} and also need to be tracked
  dynamically.
  \item \texttt{compiler/prelude/PrimOp.hs} is imported in many other source
  files, and participates in the rest of the build process in a more or less
  standard way.
\end{itemize}

\noindent Creating a separate build phase for each step in the above chain is
impractical and leads to poor performance results.

We handle dynamic dependencies using \textsc{Shake}'s monadic dependencies.

\subsection{Fine-grain dependencies}

We have the following fine-grain dependencies:
\begin{itemize}
  \item Depend on individual configuration flags provided in `system.config'
  file. When a flag is changed only the affected rules are rerun.
  \item Compute build arguments from target filename.
\end{itemize}

\textsc{Make} supports only file dependencies, so it is impossible to depend
on anything finer, e.g. a single flag in a configuration file. Keeping flags
separately in different files is impractical.

We can't use a single file with \make{} because a key point is that the whole file
may change, but the inner piece may not change. \textsc{Shake} solves this with
unchanging files.

See Section~\ref{section-dependencies} for more details.

In addition, to avoid storing many small files on disk, we use oracles which let
us store these directly in \textsc{Shake}'s database. That's not necessary, but
avoids many small files.

\subsection{Everything is a file}


\subsubsection{Real code}

Much of a build system is calling out to programs that execute real code, e.g. compilers. But there is some stuff that requires custom code, for example splitting command line arguments that exceed a certain size. In the existing build system we use xargs, which alas doesn't work consistently between different OS versions, and does both more than we want and less than we want. Fortunately, with Haskell at our disposal, we can write:

\begin{lstlisting}
-- | @chunksOfSize size strings@ splits a given list of strings into chunks not
--   exceeding @size@ characters. If that is impossible, it uses singleton chunks.
chunksOfSize :: Int -> [String] -> [[String]]
chunksOfSize n = repeatedly $ \xs ->
    let i = length $ takeWhile (<= n) $ scanl1 (+) $ map length xs
    in splitAt (max 1 i) xs
\end{lstlisting}

Writing a small function is easy in Haskell, and even easier to test (we test this function using QuickCheck). Writing it in bash would be infeasible. Reducing the specific behaviours required from external tools leads to significantly less cross-platform concerns.

\subsection{Difficulty of building a DSL}

In any large build system, there are rules (how to build things), and then a very long list of corner cases. For example,

\begin{lstlisting}
compiler_ALEX_OPTS = --latin1
\end{lstlisting}

While this is a concise way of representing it, it's inflexible. Rather than pattern matching on everything, we are pattern matching on two components (which package we are building and which tool we are running). Adding additional filters is impossible.

Implementing the DSL is a nightmare. It's really a configuration setting.


The question of provenance is particularly important in large multi-author build systems, but generally not something that is addressed anywhere - one of our key inovations in \S?. The encapsulation is provided by Haskell modules. The lack of freedom is provided by strong name binding. The problems related to `:' go away by manipulating values rather program text.


\subsection{The consequence of unnecessary complexities}

These unnecessary complexities have a big consequence in terms of complexity and performance. They obscure the underlying real stuff in the build system. Simply removing these already gets us to a much better state.

The main lessons we have learnt are:

\begin{itemize}
\item Abstraction is a powerful and necessary tool. Lack of good abstraction mechanisms is a significant cause of the complexity of previous attempts in \make{}. Functional programming provides good abstractions which in turn allow reducing the complexity at each level, while still obtaining a more powerful result. Marrying build systems and functional programming works well.
\item Use a build system that can express the necessary dependencies. We make use of \textit{monadic dependencies} \S?, \textit{non-file dependencies} \S?, \textit{resources} \S?. While some of these features are only used in a few places (e.g. resources), their absence would require pervasive workarounds, and a significant increase in overall complexity.
\end{itemize}

