\section{Challenges of large-scale build systems\label{sec:challenges}}

\todo{Shorten the problem descriptions, using the footnote as an overall 
  motivating example. Unfootnote it.}

\todo{Describe the history of previous attempts, each better than before: SDM}
\begin{enumerate}
  \item jmake - cpp + make clone of X11 imake
  \item GNU make - drop cpp, recursive make, manual stages
  \item GNU make - no manual stages, cabal integration, first
   use of macros
   \item GNU make - current iteration, non recursive, cite Recursive
     Make Considered Harmful here, extensive use of macros,
     building abstractions in make
\end{enumerate}

\subsection{The GHC build system}

Build systems need to evolve quickly with the projects they support. While it is
\textit{possible} to develop large scale systems in \make, it is not
\textit{pleasant} -- resulting in heroic efforts where straight-forward
simplicity should be preferred.

The GHC build system is a complex multi-language build system. The system is a
bootstrapping compiler, where the GHC compiler is built using a system compiler,
then recompiled using built compiler and recompiled system libraries. This
pattern naturally contains repeated patterns, but capturing them in \make{} is
hard.

We focus on the GHC build system for two reasons. Firstly, it is the coal-face
at which many of us work, so improvements to it will bring real improvements to
our daily development. Secondly, the GHC build system has many complex features
that test the limits of existing features:

\begin{itemize}
\item GHC is cross-language, including large amounts of both C and Haskell code.
It also generates user manuals from docbook, which can be viewed as another
language with unique build/dependency patterns.
\item GHC is a bootstrapping compiler with stages. It first builds a compiler
using the system compiler, then uses that new compiler
\item The GHC build system necessarily integrates with other build systems,
including \make{} (for building libgmp) and \cabal{} (for building/registering
Haskell libraries).
\item It generates files, e.g. \texttt{compiler/stage1/ghc\_boot\_platform.h}
that \texttt{\#define}'s various platform-specific constants used throughout the
build system.
\item It is cross-platform, working on Windows, Linux, Mac, iOS, Android,
Solaris, BSD flavours etc.
\item The system is in constant flux as new features are added to the system.
\end{itemize}

Writing such a build system remains a challenging engineering undertaking, but
one we now hope not to have to repeat.

The GHC build system stumbles into a number of nasty corners of \make{}. In this section we reflect on the challenges, and how they can be solved generically in our new system. We very much consider these \textit{unnecessary} complexities -- they are not consequences of our problem domain, merely weaknesses of \make{} on large projects. In general the problems can be divided into those due to the \make{} language (macros, variables etc.) and those due to the \make{} dependency features (lack of expressive dependencies). Consequently, we tackle these problems using functional programming for the language level, and Shake for the dependency level. After tackling these unnecessary complexities, we show how to construct a large build system \S\ref{sec:abstractions}.

\subsection{Programming model}

\make{}'s program state involves a global namespace of string variables which are spliced into the program. This model naturally causes challenges:

\begin{itemize}
\item Since all variables live in a single global namespace, there is limited encapsulation and implementation hiding. While macros introduce some level of scope, their use leads to different problems.
\item Excessive freedom, implementing \lst"$$$$foo" to do multiple lookups. Such code is much like \lst"eval" in a scripting language, but it is unlikely any scripting language author would encourage such a level of indirection. Alas, without real data structures (associative maps, arrays) such tricks are necessary.
\item Since the text is spliced into the resulting Makefile to be interpreted, certain characters can pose lots of problems -- notably space (which splits lexemes which might have been intended to be joined) and `:' (which declares rules, but often tends to be a Windows drive letter gone astray). Much like command line quoting, with sufficient care such issues can be avoided -- but it does require constant care.
\item A string variable, such as \lst"$1_$2_$3_HC_OPTS", can be manipulated in many different files and it is very difficult to track the provenance of a specific command line argument that is eventually passed to a command line tool.
\end{itemize}

The solution to the issue of encapsulation is a properly block-scoped language with separate units of implementation hiding, e.g. modules in Haskell. The solution to excessive freedom is types (not necessarily static). The solution to splicing is to separate values and program text, much like most programming languages do. By using Haskell, or indeed any other modern programming language, most of these issues are solve.

An alternative solution would be to avoid putting complexity into the Makefile, and place it in a generator of a Makefile (much like the Ninja build system does). Alas, such a solution runs into problems if there is a dynamic call graph, as per \S?.

At this point it is worth remarking that many alternative build systems do indeed offer real programming language embedding, e.g. SCons (Python). We address the specific limitations that drove us away from them in later sections, and discuss all alternatives in \S?.

\subsection{Pattern/rule language}

A build system builds some output files from some input files. The fundamental unit of work in \make{} (and also Shake) is a rule that produces some output by running commands on some input. Consider the following pattern rule:

\begin{lstlisting}
%.o : %.hs
    ghc $HC_OPTS $<
\end{lstlisting}

\noindent It tells \make{} that object files \lst"*.o" can be produced from Haskell source files \lst"*.hs" by compiling them with \lst"ghc" command invoked with \lst"HC_OPTS" arguments. The notation is terse and works well for this simple case. Unfortunately, this simple formulation does not support numerous important features, for example:

\begin{itemize}
\item What if we want the rule to match \lst"foo.o" and \lst"bar.o", but not \lst"baz.o"? It is impossible to do any non-trivial computation -- we are forced to rely on patterns whose expressive power is limited.
\item What if \lst"HC_OPTS" depends on the file being compiled? At the moment it is a fixed global variable.
\end{itemize}

\newsavebox{\exampleCode}
\begin{lrbox}{\exampleCode}
\begin{minipage}[t]{\columnwidth}
Here is the code in full glory. We cannot go into details due to lack of
space, but would like to point out that there are \$57 in three lines!
\begin{lstlisting}[basicstyle=\footnotesize\sffamily,escapeinside={(*}{*)}]
$1/$2/build/%.$$($3_osuf) : $1/$4/%.hs $$(LAX_DEPS_FOLLOW) \
    $$$$($1_$2_HC_DEP) $$($1_$2_PKGDATA_DEP)
  $$(call cmd,$1_$2_HC) $$($1_$2_$3_ALL_HC_OPTS) -c $$< -o \
    $$@ $$(if $$(findstring YES,$$($1_$2_DYNAMIC_TOO)),-dyno \
    $$(addsuffix .$$(dyn_osuf),$$(basename $$@)))
  $$(call ohi(*-*)sanity(*-*)check,$1,$2,$3,$1/$2/build/$$*)
\end{lstlisting}
\end{minipage}
\end{lrbox}

The standard approach to overcome some of \make{}'s limitations is to use
\emph{macros}. They provide some additional flexibility, but quickly become difficult to follow. As an example, here is a \emph{significantly simplified} snippet\footnote{\usebox{\exampleCode}} from the GHC build system:

\begin{lstlisting}
$1/$2/build/%.o : $1/$4/%.hs $$$$($1_$2_HC)
    $1_$2_HC $$($1_$2_$3_HC_OPTS) -c $$< -o $$@
\end{lstlisting}

\noindent As before, the rule is responsible for compiling a Haskell source
file into an object file. Arguments of the macro (\lst"$1" to \lst"$4") provide
additional information about the current build target so that we can pick the
right Haskell compiler \lst"$1_$2_HC" and run it with appropriate command
line arguments \lst"$1_$2_$3_HC_OPTS".

Using Shake the simplest variant looks slightly more complex as it has had Haskell syntax inserted:

\begin{lstlisting}
"*.o" %> \out -> do
    let hs = out -<.> ".hs"
    need [hs]
    cmd "ghc" hc_opts hs
\end{lstlisting}

However, as the complexity grows, so the build system scales properly. Making \lst"hc_opts" depend on the Haskell file requires the small and obvious change of:

\begin{lstlisting}
    cmd "ghc" (hc_opts hs) hs
\end{lstlisting}

To use richer pattern matching we can drop down to a lower level Shake operation. In Shake the definition of is \lst"%>" is itself defined in terms of \lst"?>", as:

\begin{verbatim}
pat %> act = (pat ?==) ?> act
\end{verbatim}

Namely the wildcard pattern matching is just a special case, so we can use an arbitrary predicate to exert more precise control over where the match occurs. In both cases the generality of higher-order functions solves the problem.

\subsection{Multiple outputs}

\todo{Make supports multiple outputs. We need to be careful about what the rules are. Is this a genuine weakness or not?}

In the rule above, GHC actually produces two files: \lst"*.o" and \lst"*.hi". How do we express this?

Mostly by pretending the .hi file depends on the .o file, which is (mostly) true. But only really because we always touch the .o file.

Using Shake we can express this pattern directly (it actually turns out to be a consequence of the more powerful dependency model).

\subsection{Reducing concurrency}

Usually we want the build system to run as many steps as possible in parallel at
any point. However, for some actions, executing multiple steps in parallel can
cause problems. As one example, in GHC packages need to be registered by
invoking the \prog{ghc-pkg} utility. This utility mutates the global state
(package database) and hence at most one package can be registered at a time, or
the database is corrupted. As another example, when using Microsoft Excel via
the scripting API a hidden Excel process is spawned and used for communication.
Running two programs against the scripting API can result in actions applying to
the wrong instance.

In \make{}, the way to reduce concurrency is to introduce fake \emph{concurrency
reduction} dependencies. In the GHC build system there are 25 packages that
require registration, and they depend on each other in a chain, to ensure one
completes before the next starts. This pattern works, but is inelegant.

There are really three problems. By over-sequentialising things we limit other
parallelism that may be available in the system. We cannot say that all pieces
should be run single-threaded, we must over-constrain to give a precise order in
which they can build. If one of the later pieces is available to build, we may
stop it by waiting for another. Finally, it's easy to miss such dependencies as
they are not centralised. By accidentally turning the distributed line into a
tree we can end up with surprising failures.

The solution is Shake is to use the \emph{resources} feature. This feature limits parallelism by declaring that a certain task requires some quantity of a global resource. For example:

\begin{lstlisting}[basicstyle=\ttfamily]
packageDb <- newResource "package-db" 1
action $ withResource packageDb 1 $ cmd "ghc-pkg" ...
\end{lstlisting}

This snippet declares a global resource named \lst"packageDb" with quantity 1,
then calls \lst"withResource" asking for a single quantity before running the
program \prog{ghc-pkg}. Provided all \prog{ghc-pkg} calls are suitably wrapped,
we will never run two instances in parallel. Furthermore, thanks to the
availability of functions, we can abstract a function that executes
\prog{ghc-pkg} and applies the resource automatically, see
\S\ref{sec:abstraction} for how we do that.

Note that build systems already schedule tasks to satisfy the implicit thread
resource, so having user defined resources is an obvious generalisation.

The only other build system we are aware of that contains resources is
\prog{Ninja}, which calls them pools.

\subsection{Dynamic dependencies}

The \make{} approach builds a dependency graph and executes it in an order satifying the dependencies. This approach is shared by most build systems. However, for large build systems, the dependency graph is not static and is not known in advance. As an example in GHC:

\begin{itemize}
  \item File \texttt{compiler/prelude/primops.txt.pp} describes GHC's primitive
  operations and types. It uses C preprocessor directives like
  \texttt{\#include} and \texttt{\#if} and therefore needs to be preprocessed.
  The result goes into \texttt{compiler/stage1/build/primops.txt}. If one of the
  included files changes the result must be rebuilt. Note: we do not know in
  advance which files are included, so we cannot depend on them statically,
  that is why such dependencies are called dynamic.
  \item File \texttt{primops.txt} is processed by \texttt{genprimopcode} utility
  to generate \texttt{primop-out-of-line.hs-incl}. \texttt{genprimopcode} itself
  needs to be built. It is a Haskell program, so it may contain \texttt{import}
  directives, which also give rise to dynamic dependencies.
  \item A dozen of \texttt{*.hs-incl} files are included (by a C preprocessor)
  into \texttt{compiler/prelude/PrimOp.hs} and also need to be tracked
  dynamically.
  \item \texttt{compiler/prelude/PrimOp.hs} is imported in many other source
  files, and participates in the rest of the build process in a more or less
  standard way.
\end{itemize}

In essence, the problem is that \lst"import" and \lst"include" statements induce dependencies, but these dependency statements are themselves generated by the build system, so the build system cannot know the dependency graph in advance.

In \make{} there are workarounds for this problem, but they are all ugly. One is to over-approximate the possible set of dependencies, but in many ways this relies on the contents of the generator, and is thus brittle to generator changes. Another approach is to divide the build into phases, where each phase is static, and the phases are run sequentially. Such a global phase ordering complicates the build and breaks compositionality.

\todo{How does the existing GHC build system solve this?}

In Shake the problem of dynamic dependencies is solved by providing the \lst"need" function, which can introduce new dependencies \emph{after observing the results of previous dependencies}. There is no graph constructed in advance.  After generating \lst"primop-out-of-line.hs-incl" we read the file to see what it imports, requiring no conservative approximations or phases.

While the existence of \lst"need" fundamentally alters the expressive power of Shake, for users it is merely a relaxation of an unnecessary rule (that \lst"need" must be the in a rule), so just makes things easier. At the build system level it requires many changes, the lack of a static dependency graph, termination checking as it executes, the ability to suspend partially executed rules etc. Fortunately, as users we do not need to care.

\textit{Aside/Footnote}: It has been suggested that the dependency mechanism in Shake should rightly be called Monadic dependencies to contrast with the Applicative dependencies in Make. We agree. In an applicative computation the structure cannot depend on the values flowing through container. In a monadic computation the structure can depend on the values.

Note that Redo is the only other clearly monadic build system, while it can be encoded in SCons using the dependency rules.

\subsection{Splitting dependencies}

There are many instances where an action depends on a subset of a file. As a compelling example in GHC, there are many indiviudal configuration flags provided in a file \lst"system.config", which reads:

\todo{include an example}

\begin{lstlisting}
system-ghc = /usr/bin/ghc
system-gcc = /usr/bin/gcc
\end{lstlisting}

When executing a command we typically depend on only a handful of lines in this large configuration file. However, using simple file dependencies, we have to depend on the whole file. As a result, we end up rebuilding more than is necessary.

One solution is to create a single file for each setting, but using \prog{configure} that is challenging. Since a single file is being generated, the next best solution is to chop up the file into multiple small files, to provide accurate fine-grained dependencies.

Taking this approach doesn't work in \make{}. We can produce individual files, but the individual files must depend on the large file, so they update appropriately. In the rule to generate the fragments, if we always generate the new file, then we do not break the dependencies -- everything still has an ultimate dependency on the whole file. If in a rule we chose to avoid generating the small file then the file always looks dirty (since its timestamp is older than the big file, which \make{} considers to signify dirty).

Fortunately, Shake has an \emph{unchanging files} feature which solves this problem. Using the same approach in Shake we can simply avoid writing out the small file if it has not changed. Shake considers a file to be rebuilt if the rule runs successfully, but only considers the file \emph{changed} if its value changed. As a result, Shake can start down a build tree and then later avoid that build tree. This feature is built into the core of Shake, and means that, unlike \make{}, we can split a single dependency into fine-grained dependencies successfully.

The results of such a change can be dramatic -- on certain real build systems we regularly see build times of a couple of seconds, which would have been minutes without this feature. This feature is also available in Tup and Ninja, although in both cases requires explicitly enabling per rule (using \texttt{stat} in Ninja, and \texttt{\^} in Tup).

\subsection{Everything is a file}

Once fine grained dependencies are available, they can quickly become pervasive. Using a build system based on filenames as keys and files as values we effectively reach the global store problem we criticised in \S? (particularly if we consider dyanmic dependencies from \S? to be \lst"$" dereferencing!). While not critical, Shake supports an Oracle feature, to allow us to make the keys and values arbitrary Haskell types, and store them in the main Shake database. Thanks to such a technique we can have an oracle per rule we create to track changing information, without necessarily doubling the number of files present in the build system. As an example:

\begin{lstlisting}
newtype ConfigKey = ConfigKey String
    deriving (Show,Typeable,Eq,Hashable,Binary,NFData)
rules = do
    addOracle $ \(ConfigKey x) -> do
        src <- readFile' "system.config"
        Map.lookup (parseFile src) x

    ... askOracle (ConfigKey x) ...
\end{lstlisting}

Here we define our own rule of type \lst"ConfigKey" which reads from the configuration and stores just that one field. Since this also participates in unchanging rules we automatically get rebuild avoidance if the value does not change.

\subsubsection{Real code}

Much of a build system is calling out to programs that execute real code, e.g. compilers. But there is some stuff that requires custom code, for example splitting command line arguments that exceed a certain size. In the existing build system we use xargs, which alas doesn't work consistently between different OS versions, and does both more than we want and less than we want. Fortunately, with Haskell at our disposal, we can write:

\begin{lstlisting}
-- | @chunksOfSize size strings@ splits a given list of strings
-- into chunks not exceeding @size@ characters. If that is
-- impossible, it uses singleton chunks.
chunksOfSize :: Int -> [String] -> [[String]]
chunksOfSize n = repeatedly $ \xs ->
    let i = length $ takeWhile (<= n) $ scanl1 (+) $ map length xs
    in splitAt (max 1 i) xs
\end{lstlisting}

Writing a small function is easy in Haskell, and even easier to test (we test this function using QuickCheck). Writing it in bash would be infeasible. Reducing the specific behaviours required from external tools leads to significantly less cross-platform concerns.

\subsection{Difficulty of building a DSL}

In any large build system, there are rules (how to build things), and then a very long list of corner cases. For example,

\begin{lstlisting}
compiler_ALEX_OPTS = --latin1
\end{lstlisting}

While this is a concise way of representing it, it's inflexible. Rather than pattern matching on everything, we are pattern matching on two components (which package we are building and which tool we are running). Adding additional filters is impossible.

Implementing the DSL is a nightmare. It's really a configuration setting.


The question of provenance is particularly important in large multi-author build systems, but generally not something that is addressed anywhere - one of our key inovations in \S?. The encapsulation is provided by Haskell modules. The lack of freedom is provided by strong name binding. The problems related to `:' go away by manipulating values rather program text.


\subsection{The consequence of unnecessary complexities}

These unnecessary complexities have a big consequence in terms of complexity and performance. They obscure the underlying real stuff in the build system. Simply removing these already gets us to a much better state.

The main lessons we have learnt are:

\begin{itemize}
\item Abstraction is a powerful and necessary tool. Lack of good abstraction mechanisms is a significant cause of the complexity of previous attempts in \make{}. Functional programming provides good abstractions which in turn allow reducing the complexity at each level, while still obtaining a more powerful result. Marrying build systems and functional programming works well.
\item Use a build system that can express the necessary dependencies. We make use of \textit{monadic dependencies} \S?, \textit{non-file dependencies} \S?, \textit{resources} \S?. While some of these features are only used in a few places (e.g. resources), their absence would require pervasive workarounds, and a significant increase in overall complexity.
\end{itemize}

